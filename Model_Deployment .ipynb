{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive to the Colab environment"
      ],
      "metadata": {
        "id": "_OLr1TtDhZf_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EClvZiOnpWb9",
        "outputId": "5c550d01-ce7d-46bd-b4c0-35e827e464ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Environment Setup and Library Installation\n"
      ],
      "metadata": {
        "id": "KZivx1EYhzU5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck-XnPVKnZ_R",
        "outputId": "23503cef-104e-47de-f7fb-c2883703bd12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.25.2)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56441 sha256=ed24b608eed9c17b5fc7af523a2f6d734480e89f453265c34a8c1f28ce89145b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n",
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2024.6.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Installing collected packages: wfdb\n",
            "Successfully installed wfdb-4.1.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.35.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.35.0 watchdog-4.0.1\n",
            "Collecting streamlit-option-menu\n",
            "  Downloading streamlit_option_menu-0.3.13-py3-none-any.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.4/823.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: streamlit>=0.63 in /usr/local/lib/python3.10/dist-packages (from streamlit-option-menu) (1.35.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (8.3.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (6.3.3)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.0.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-option-menu) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-option-menu) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-option-menu) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-option-menu) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-option-menu) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-option-menu) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-option-menu) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-option-menu) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.63->streamlit-option-menu) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.63->streamlit-option-menu) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-option-menu) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-option-menu) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=0.63->streamlit-option-menu) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit>=0.63->streamlit-option-menu) (1.16.0)\n",
            "Installing collected packages: streamlit-option-menu\n",
            "Successfully installed streamlit-option-menu-0.3.13\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install np_utils\n",
        "! pip install wfdb\n",
        "! pip install tensorflow\n",
        "! pip install streamlit\n",
        "! pip install streamlit-option-menu\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cloud Setup"
      ],
      "metadata": {
        "id": "C6-R1on-jA-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import keras.backend as K"
      ],
      "metadata": {
        "id": "DKAGLHW_50Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMA20me17nFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb9a5040-70d9-4463-da38-d317a608f091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "\n",
        "# Replace 'your_auth_token' with your actual ngrok auth token\n",
        "ngrok.set_auth_token(\"2hWXK2xfX2Xyzbd78hlWuGMhSdO_3Wmg1JW7oC61RyrhY2ggJ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0lgfuDiywQK",
        "outputId": "a23f5e53-e5a6-4396-9823-850a85e0bfdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.185.28.111\n"
          ]
        }
      ],
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "tunnels = ngrok.get_tunnels()\n",
        "for tunnel in tunnels:\n",
        "    print(tunnel.public_url)"
      ],
      "metadata": {
        "id": "n-ApXhr6Ow5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'http://<tunnel_id>.ngrok.io' with the URL of the tunnel you want to stop\n",
        "ngrok.disconnect('https://d8c6-34-83-251-184.ngrok-free.app')"
      ],
      "metadata": {
        "id": "MK9V8y3LO4tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Web App Using Streamlit"
      ],
      "metadata": {
        "id": "y8hr_ZjgipEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dense, Dropout, MaxPooling1D, Activation, BatchNormalization, Lambda, TimeDistributed\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv1D, AvgPool1D, Flatten, Dropout, Dense, Softmax\n",
        "from keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "import seaborn as sns\n",
        "import streamlit as st\n",
        "import pywt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from streamlit_option_menu import option_menu\n",
        "from scipy import stats\n",
        "from tensorflow.keras.models import load_model\n",
        "from wfdb import rdrecord,rdann,rdsamp\n",
        "import os\n",
        "from scipy.signal import find_peaks\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "\n",
        "def zeropad(x):\n",
        "    y = K.zeros_like(x)\n",
        "    return K.concatenate([x, y], axis=2)\n",
        "\n",
        "def zeropad_output_shape(input_shape):\n",
        "    shape = list(input_shape)\n",
        "    assert len(shape) == 3\n",
        "    shape[2] *= 2\n",
        "    return tuple(shape)\n",
        "\n",
        "def ECG_model():\n",
        "    \"\"\"\n",
        "    Implementation of the model in https://www.nature.com/articles/s41591-018-0268-3\n",
        "    Also referring to codes at\n",
        "    https://github.com/awni/ecg/blob/master/ecg/network.py\n",
        "    and\n",
        "    https://github.com/fernandoandreotti/cinc-challenge2017/blob/master/deeplearn-approach/train_model.py\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional block\n",
        "    model.add(Conv1D(filters=32, kernel_size=16, padding='same', kernel_initializer='he_normal', input_shape=(256, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling1D(pool_size=1, strides=1))\n",
        "\n",
        "    model.add(Conv1D(filters=32, kernel_size=16, padding='same', kernel_initializer='he_normal'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(filters=32, kernel_size=16, padding='same', kernel_initializer='he_normal'))\n",
        "\n",
        "    # Main loop blocks\n",
        "    filter_length = 32\n",
        "    n_blocks = 15\n",
        "    for block_index in range(n_blocks):\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Conv1D(filters=filter_length, kernel_size=16, padding='same', kernel_initializer='he_normal'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv1D(filters=filter_length, kernel_size=16, padding='same', kernel_initializer='he_normal'))\n",
        "        if block_index % 2 == 0:\n",
        "            model.add(MaxPooling1D(pool_size=2))\n",
        "        if block_index % 4 == 0 and block_index > 0:\n",
        "            model.add(Lambda(zeropad, output_shape=zeropad_output_shape))\n",
        "            filter_length *= 2\n",
        "\n",
        "    # Output block\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    # adam = Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\n",
        "# Compile the model\n",
        "\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "x_signals1 = []\n",
        "y_signals1 = []\n",
        "# 1. Sidebar menu\n",
        "with st.sidebar:\n",
        "    selected = option_menu(\"NY Cardio\", [\"Signal Prediction\", 'Model Training', 'Model Evaluation'],\n",
        "                           icons=['signal', 'gear', 'check'], menu_icon=\"heart\", default_index=0)\n",
        "\n",
        "# Denoising function using Wavelet Transform\n",
        "def denoise(data):\n",
        "    w = pywt.Wavelet('sym4')\n",
        "    maxlev = pywt.dwt_max_level(len(data), w.dec_len)\n",
        "    threshold = 0.04  # Threshold for filtering\n",
        "\n",
        "    coeffs = pywt.wavedec(data, 'sym4', level=maxlev)\n",
        "    for i in range(1, len(coeffs)):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], threshold * max(coeffs[i]))\n",
        "\n",
        "    datarec = pywt.waverec(coeffs, 'sym4')\n",
        "    return datarec\n",
        "# \"\"\"************************** train Model Part***************************************************\"\"\"\n",
        "\n",
        "\n",
        "def save_uploaded_file(uploaded_file, upload_dir):\n",
        "    file_path = os.path.join(upload_dir, uploaded_file.name)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(uploaded_file.read())\n",
        "    return file_path\n",
        "\n",
        "if selected == \"Model Training\":\n",
        "    classes = ['N', 'F', 'Q', 'S', 'V']\n",
        "    class_descriptions = {\n",
        "        'N': 'Nonectopic beats',\n",
        "        'F': 'Fusion beats',\n",
        "        'Q': 'Unknown beats',\n",
        "        'S': 'Supraventricular ectopic beats',\n",
        "        'V': 'Ventricular ectopic beats'\n",
        "    }\n",
        "    example_beat_printed = False\n",
        "    example_beat_normalized_printed = False\n",
        "    UPLOAD_DIR = \"uploads\"\n",
        "    os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "\n",
        "    st.write(\"## **Model Training**\")\n",
        "\n",
        "    st.write(\"#### **Train Data**\")\n",
        "    train_dat_file = st.file_uploader(\"Choose a .dat or .mat file\", type=['dat', 'mat'])\n",
        "    train_hea_file = st.file_uploader(\"Choose a .hea file\", type=['hea'])\n",
        "    train_atr_file = st.file_uploader(\"Choose a .atr file\", type=['atr'])\n",
        "\n",
        "    if train_dat_file and train_hea_file and train_atr_file:\n",
        "        file_location_dat = save_uploaded_file(train_dat_file, UPLOAD_DIR)\n",
        "        file_location_hea = save_uploaded_file(train_hea_file, UPLOAD_DIR)\n",
        "        file_location_atr = save_uploaded_file(train_atr_file, UPLOAD_DIR)\n",
        "\n",
        "        base_file_location_hea = file_location_hea.replace('.hea', '')\n",
        "        base_file_location_atr = file_location_atr.replace('.atr', '')\n",
        "\n",
        "        st.write(\"### **Preprocessing** \")\n",
        "        record = rdrecord(base_file_location_hea, smooth_frames=True)\n",
        "        signals1 = np.nan_to_num(record.p_signal[:, 0]).tolist()\n",
        "\n",
        "        denoised_signals1 = denoise(signals1)\n",
        "        st.write(\"#### **Phase 1: Data Denoising**\")\n",
        "\n",
        "        def plot_signal(signal, title, filename):\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.title(title)\n",
        "            plt.plot(signal, linewidth=2)\n",
        "            plt.ylabel('Amplitude')\n",
        "            plt.xlabel('Sample number')\n",
        "            plt.savefig(filename)\n",
        "            plt.close()\n",
        "            return filename\n",
        "\n",
        "        noised_signal_plot = plot_signal(signals1[0:256*3], \"Signal before denoising\", \"noised_signal_plot.png\")\n",
        "        denoised_signal_plot = plot_signal(denoised_signals1[0:256*3], \"Signal after denoising\", \"denoised_signal_plot.png\")\n",
        "\n",
        "        denoising_expander1 = st.expander(label='Data denoising')\n",
        "        with denoising_expander1:\n",
        "            st.image(noised_signal_plot, caption='Noised Signal')\n",
        "            st.image(denoised_signal_plot, caption='Denoised Signal')\n",
        "\n",
        "        with st.spinner(\"Executing ...\"):\n",
        "            peaks_signals1, _ = find_peaks(denoised_signals1, distance=150)\n",
        "            x_signals1, y_signals1 = [], []\n",
        "\n",
        "            for peak in peaks_signals1[1:-1]:\n",
        "                start, end = peak - 128, peak + 128\n",
        "                if end > len(denoised_signals1):\n",
        "                    continue\n",
        "                wave = denoised_signals1[start:end]\n",
        "                ann = rdann(base_file_location_atr, extension='atr', sampfrom=start, sampto=end)\n",
        "                annSymbol = ann.symbol\n",
        "\n",
        "                if not example_beat_printed:\n",
        "                    segmented_heartbeat_plot = plot_signal(wave, \"Segmented heartbeat\", \"segmented_heartbeat.png\")\n",
        "                    example_beat_printed = True\n",
        "\n",
        "                wave = stats.zscore(wave)\n",
        "                if not example_beat_normalized_printed:\n",
        "                    normalized_heartbeat_plot = plot_signal(wave, \"Heartbeat after z-score normalization\", \"normalized_heartbeat.png\")\n",
        "                    example_beat_normalized_printed = True\n",
        "\n",
        "                if len(annSymbol) == 1 and annSymbol[0] in classes:\n",
        "                    arrhythmia_index = classes.index(annSymbol[0])\n",
        "                    x_signals1.append(wave)\n",
        "                    y_signals1.append(arrhythmia_index)\n",
        "\n",
        "            st.write(\"#### **Phase 2: Heartbeat Signal Extraction**\")\n",
        "            leads_expander1 = st.expander(label='Heartbeat Signal Extraction')\n",
        "            with leads_expander1:\n",
        "                st.image(segmented_heartbeat_plot, caption='Segmented heartbeat')\n",
        "\n",
        "            st.write(\"#### **Phase 3: Normalization**\")\n",
        "            normalization_expander = st.expander(label='Normalization')\n",
        "            with normalization_expander:\n",
        "                st.image(normalized_heartbeat_plot, caption='Normalized heartbeat')\n",
        "\n",
        "            st.write(\"#### **Phase 4: Mapping MIT Dataset Classes to AAMI Categories**\")\n",
        "            data1_expander = st.expander(label='Number of samples after the split')\n",
        "            with data1_expander:\n",
        "                st.write(f\"All Data: {len(x_signals1)}\")\n",
        "\n",
        "        x_signals1, y_signals1 = np.array(x_signals1), np.array(y_signals1)\n",
        "        train_x, val_x, train_y, val_y = train_test_split(x_signals1, y_signals1, test_size=0.20, random_state=42)\n",
        "        st.write(\"#### **Phase 5: Split Data into Test and Validation**\")\n",
        "\n",
        "        data2_expander = st.expander(label='Number of samples after the split')\n",
        "        with data2_expander:\n",
        "            st.write(f\"Train Data: {len(train_x)}\")\n",
        "            st.write(f\"Validation Data: {len(val_x)}\")\n",
        "\n",
        "        st.write(\"#### **Phase 6: Oversampling**\")\n",
        "        oversampler = RandomOverSampler()\n",
        "        train_x_resampled, train_y_resampled = oversampler.fit_resample(train_x, train_y)\n",
        "\n",
        "        data3_expander = st.expander(label='Train Data after Oversampling')\n",
        "        with data3_expander:\n",
        "            st.write(f\"Train Data after Oversampling: {len(train_x_resampled)}\")\n",
        "\n",
        "        st.write(\"### **Model Training** \")\n",
        "        st.write(\"#### **Enter Training Parameters**\")\n",
        "        with st.form(key='training_form'):\n",
        "            learning_rate = st.number_input('Learning Rate', min_value=1e-6, max_value=1.0, value=0.001, step=1e-6)\n",
        "            epochs = st.number_input('Epochs', min_value=1, max_value=100, value=5, step=1)\n",
        "            batch_size = st.number_input('Batch Size', min_value=1, max_value=1024, value=185, step=1)\n",
        "            loss_function = st.selectbox('Loss Function', ['categorical_crossentropy', 'sparse_categorical_crossentropy'])\n",
        "            optimizer = st.selectbox('Optimizer', ['adam', 'sgd', 'rmsprop'])\n",
        "            submit_button = st.form_submit_button(label='Submit')\n",
        "\n",
        "        if submit_button:\n",
        "            with st.spinner(\"Training ...\"):\n",
        "                callbacks = [\n",
        "                    EarlyStopping(patience=10, verbose=1),\n",
        "                    ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.01, verbose=1),\n",
        "                    TensorBoard(log_dir='./logs/fold_APP', histogram_freq=0, write_graph=True, write_grads=False, write_images=True),\n",
        "                    ModelCheckpoint('/content/drive/MyDrive/MLII-latest_v1_fold_APP.hdf5', monitor='val_loss', save_best_only=False, verbose=1, period=10)\n",
        "                ]\n",
        "\n",
        "                # One-hot encode labels\n",
        "                train_y_resampled_cat = to_categorical(train_y_resampled, num_classes=len(classes))\n",
        "                val_y_cat = to_categorical(val_y, num_classes=len(classes))\n",
        "\n",
        "                # Create and compile the model\n",
        "                model = ECG_model()\n",
        "\n",
        "                # Set optimizer\n",
        "                if optimizer == 'adam':\n",
        "                    opt = Adam(learning_rate=learning_rate)\n",
        "                elif optimizer == 'sgd':\n",
        "                    opt = SGD(learning_rate=learning_rate)\n",
        "                elif optimizer == 'rmsprop':\n",
        "                    opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "                # Set loss function\n",
        "                if loss_function == 'categorical_crossentropy':\n",
        "                    loss_fn = categorical_crossentropy\n",
        "                elif loss_function == 'sparse_categorical_crossentropy':\n",
        "                    loss_fn = sparse_categorical_crossentropy\n",
        "\n",
        "                model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "                history = model.fit(train_x_resampled, train_y_resampled_cat,\n",
        "                                    validation_data=(val_x, val_y_cat),\n",
        "                                    epochs=epochs,\n",
        "                                    batch_size=batch_size,\n",
        "                                    callbacks=callbacks,\n",
        "                                    initial_epoch=0)\n",
        "                st.write(\"#### **Accuracy and Loss Curve**\")\n",
        "                train_expander = st.expander(label='Accuracy and Loss Curve')\n",
        "                with train_expander:\n",
        "                    def plot_training_history(history, metric, title, filename):\n",
        "                        plt.figure(figsize=(12, 6))\n",
        "                        plt.plot(history.history[metric], label=f'Training {metric}')\n",
        "                        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric}')\n",
        "                        plt.title(title)\n",
        "                        plt.xlabel('Epoch')\n",
        "                        plt.ylabel(metric.capitalize())\n",
        "                        plt.legend()\n",
        "                        plt.savefig(filename)\n",
        "                        plt.close()\n",
        "                        return filename\n",
        "\n",
        "                    loss_curve = plot_training_history(history, 'loss', 'Loss Curve', 'loss_curve.png')\n",
        "                    accuracy_curve = plot_training_history(history, 'accuracy', 'Accuracy Curve', 'accuracy_curve.png')\n",
        "\n",
        "                    st.image(loss_curve, caption='Loss Curve')\n",
        "                    st.image(accuracy_curve, caption='Accuracy Curve')\n",
        "                st.write(\"#### **Save form**\")\n",
        "                with st.form(key='save_form'):\n",
        "                    model_name = st.text_input('Enter model name (without .h5 extension):')\n",
        "                    save_button = st.form_submit_button(label='Save')\n",
        "\n",
        "                if save_button and model_name:\n",
        "                    model.save(f\"{model_name}.h5\")\n",
        "                    st.success(f\"Model saved as {model_name}.h5\")\n",
        "# \"\"\"**************************  Evaluation Part***************************************************\"\"\"\n",
        "if selected == \"Model Evaluation\":\n",
        "    st.write(\"## **Model Evaluation**\")\n",
        "    classes = ['N', 'F', 'Q', 'S', 'V']\n",
        "    class_descriptions = {\n",
        "        'N': 'Nonectopic beats',\n",
        "        'F': 'Fusion beats',\n",
        "        'Q': 'Unknown beats',\n",
        "        'S': 'Supraventricular ectopic beats',\n",
        "        'V': 'Ventricular ectopic beats'\n",
        "    }\n",
        "    example_beat_printed = False\n",
        "    example_beat_normalized_printed = False\n",
        "    UPLOAD_DIR = \"uploads\"\n",
        "    MODEL_DIR = \"/content/drive/MyDrive\"\n",
        "    os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "    x_signals1, y_signals1 = [], []\n",
        "    # Scan for .h5 files in the MODEL_DIR\n",
        "    model_files = [f for f in os.listdir(MODEL_DIR) if f.endswith('.h5')]\n",
        "    st.write(\"#### **Select Model**\")\n",
        "    # Model selection dropdown\n",
        "    selected_model_file = st.selectbox(\"Choose a model for prediction\", model_files)\n",
        "    st.write(\"#### **Test Data**\")\n",
        "    train_dat_file = st.file_uploader(\"Choose a .dat or .mat file\", type=['dat', 'mat'])\n",
        "    train_hea_file = st.file_uploader(\"Choose a .hea file\", type=['hea'])\n",
        "    train_atr_file = st.file_uploader(\"Choose a .atr file\", type=['atr'])\n",
        "\n",
        "    if train_dat_file and train_hea_file and train_atr_file:\n",
        "        file_location_dat = save_uploaded_file(train_dat_file, UPLOAD_DIR)\n",
        "        file_location_hea = save_uploaded_file(train_hea_file, UPLOAD_DIR)\n",
        "        file_location_atr = save_uploaded_file(train_atr_file, UPLOAD_DIR)\n",
        "\n",
        "        base_file_location_hea = file_location_hea.replace('.hea', '')\n",
        "        base_file_location_atr = file_location_atr.replace('.atr', '')\n",
        "\n",
        "        st.write(\"### **Preprocessing** \")\n",
        "        record = rdrecord(base_file_location_hea, smooth_frames=True)\n",
        "        signals1 = np.nan_to_num(record.p_signal[:, 0]).tolist()\n",
        "\n",
        "        denoised_signals1 = denoise(signals1)\n",
        "        st.write(\"#### **Phase 1: Data Denoising**\")\n",
        "\n",
        "        def plot_signal(signal, title, filename):\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.title(title)\n",
        "            plt.plot(signal, linewidth=2)\n",
        "            plt.ylabel('Amplitude')\n",
        "            plt.xlabel('Sample number')\n",
        "            plt.savefig(filename)\n",
        "            plt.close()\n",
        "            return filename\n",
        "\n",
        "        noised_signal_plot = plot_signal(signals1[0:256*3], \"Signal before denoising\", \"noised_signal_plot.png\")\n",
        "        denoised_signal_plot = plot_signal(denoised_signals1[0:256*3], \"Signal after denoising\", \"denoised_signal_plot.png\")\n",
        "\n",
        "        denoising_expander1 = st.expander(label='Data denoising')\n",
        "        with denoising_expander1:\n",
        "            st.image(noised_signal_plot, caption='Noised Signal')\n",
        "            st.image(denoised_signal_plot, caption='Denoised Signal')\n",
        "\n",
        "        with st.spinner(\"Executing ...\"):\n",
        "            peaks_signals1, _ = find_peaks(denoised_signals1, distance=150)\n",
        "\n",
        "\n",
        "            for peak in peaks_signals1[1:-1]:\n",
        "                start, end = peak - 128, peak + 128\n",
        "\n",
        "                wave = denoised_signals1[start:end]\n",
        "                ann = rdann(base_file_location_atr, extension='atr', sampfrom=start, sampto=end)\n",
        "                annSymbol = ann.symbol\n",
        "\n",
        "\n",
        "                if not example_beat_printed:\n",
        "                    segmented_heartbeat_plot = plot_signal(wave, \"Segmented heartbeat\", \"segmented_heartbeat.png\")\n",
        "                    example_beat_printed = True\n",
        "\n",
        "                wave = stats.zscore(wave)\n",
        "                if not example_beat_normalized_printed:\n",
        "                    normalized_heartbeat_plot = plot_signal(wave, \"Heartbeat after z-score normalization\", \"normalized_heartbeat.png\")\n",
        "                    example_beat_normalized_printed = True\n",
        "                if len(annSymbol) ==1 and (annSymbol[0] in classes) :\n",
        "                  if annSymbol[0] == 'F':\n",
        "                    arrhythmia_index = classes.index('F')\n",
        "                  elif annSymbol[0] == 'N' or annSymbol[0] == 'L' or annSymbol[0] == 'R' or annSymbol[0] == 'e' or annSymbol[0] == 'j':\n",
        "                    arrhythmia_index = classes.index('N')\n",
        "                  elif annSymbol[0] == '/' or annSymbol[0] == 'f' or annSymbol[0] == 'U':\n",
        "                    arrhythmia_index = classes.index('Q')\n",
        "                  elif annSymbol[0] == 'A' or annSymbol[0] == 'a' or annSymbol[0] == 'J' or annSymbol[0] == 'S':\n",
        "                    arrhythmia_index = classes.index('S')\n",
        "                  elif annSymbol[0] == 'V' or annSymbol[0] == 'E':\n",
        "                    arrhythmia_index = classes.index('V')\n",
        "                  x_signals1.append(wave)\n",
        "                  y_signals1.append(arrhythmia_index)\n",
        "\n",
        "            st.write(\"#### **Phase 2: Heartbeat Signal Extraction**\")\n",
        "            leads_expander1 = st.expander(label='Heartbeat Signal Extraction')\n",
        "            with leads_expander1:\n",
        "                st.image(segmented_heartbeat_plot, caption='Segmented heartbeat')\n",
        "\n",
        "            st.write(\"#### **Phase 3: Normalization**\")\n",
        "            normalization_expander = st.expander(label='Normalization')\n",
        "            with normalization_expander:\n",
        "                st.image(normalized_heartbeat_plot, caption='Normalized heartbeat')\n",
        "\n",
        "            st.write(\"#### **Phase 4: Mapping MIT Dataset Classes to AAMI Categories**\")\n",
        "            data1_expander = st.expander(label='Number of samples after the split')\n",
        "            with data1_expander:\n",
        "                st.write(f\"All Data: {len(x_signals1)}\")\n",
        "\n",
        "        x_signals1 = np.array(x_signals1)\n",
        "        st.write(\"### **Classification** \")\n",
        "        # Load the model\n",
        "        selected_model_path = os.path.join(MODEL_DIR, selected_model_file)\n",
        "        model = load_model(selected_model_path)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        test_x = np.array(x_signals1).reshape(-1, 256, 1)\n",
        "        test_predictions = model.predict(test_x)\n",
        "        test_predictions = np.argmax(test_predictions, axis=1)\n",
        "        predicted_labels = [classes[pred] for pred in test_predictions]\n",
        "\n",
        "        # Get unique classes\n",
        "        unique_classes = list(set(predicted_labels))\n",
        "        classification_expander = st.expander(label='Classification')\n",
        "        with classification_expander:\n",
        "            st.write(\"This ECG contains the following arrhythmias:\")\n",
        "            for label in unique_classes:\n",
        "                st.write(f\"- {class_descriptions[label]} ({label})\")\n",
        "                    # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_signals1, test_predictions)\n",
        "\n",
        "\n",
        "        # Calculate F1 score\n",
        "        f1 = f1_score(y_signals1, test_predictions, average='weighted')\n",
        "        st.write(\"### **accuracy and f1_score** \")\n",
        "        data12_expander = st.expander(label='accuracy and f1_score ')\n",
        "        with data12_expander:\n",
        "          st.write(f\"accuracy: {accuracy*100}%\")\n",
        "          st.write(f\"f1_score: {f1*100}%\")\n",
        "\n",
        "        conf_matrix = confusion_matrix(y_signals1, test_predictions)\n",
        "        # Calculate confusion matrix with percentages\n",
        "        conf_matrix_percentage = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        # Plot both confusion matrices\n",
        "        plt.figure(figsize=(16, 8))\n",
        "\n",
        "        # Plot confusion matrix with counts\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "        plt.title('Confusion Matrix ')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.savefig(\"matrix.png\")\n",
        "        plt.close()\n",
        "\n",
        "# \"\"\"************************** Signal Prediction Part***************************************************\"\"\"\n",
        "if selected == \"Signal Prediction\":\n",
        "    st.write(\"## **Signal Prediction**\")\n",
        "    classes = ['N', 'F', 'Q', 'S', 'V']\n",
        "    class_descriptions = {\n",
        "        'N': 'Nonectopic beats',\n",
        "        'F': 'Fusion beats',\n",
        "        'Q': 'Unknown beats',\n",
        "        'S': 'Supraventricular ectopic beats',\n",
        "        'V': 'Ventricular ectopic beats'\n",
        "    }\n",
        "    example_beat_printed = False\n",
        "    example_beat_normalized_printed = False\n",
        "    UPLOAD_DIR = \"uploads\"\n",
        "    MODEL_DIR = \"/content/drive/MyDrive\"\n",
        "    os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "    x_signals1, y_signals1 = [], []\n",
        "    # Scan for .h5 files in the MODEL_DIR\n",
        "    model_files = [f for f in os.listdir(MODEL_DIR) if f.endswith('.h5')]\n",
        "    st.write(\"#### **Select Model**\")\n",
        "    # Model selection dropdown\n",
        "    selected_model_file = st.selectbox(\"Choose a model for prediction\", model_files)\n",
        "    st.write(\"#### **Test Data**\")\n",
        "    train_dat_file = st.file_uploader(\"Choose a .dat or .mat file\", type=['dat', 'mat'])\n",
        "    train_hea_file = st.file_uploader(\"Choose a .hea file\", type=['hea'])\n",
        "    train_atr_file = st.file_uploader(\"Choose a .atr file\", type=['atr'])\n",
        "\n",
        "    if train_dat_file and train_hea_file and train_atr_file:\n",
        "        file_location_dat = save_uploaded_file(train_dat_file, UPLOAD_DIR)\n",
        "        file_location_hea = save_uploaded_file(train_hea_file, UPLOAD_DIR)\n",
        "        file_location_atr = save_uploaded_file(train_atr_file, UPLOAD_DIR)\n",
        "\n",
        "        base_file_location_hea = file_location_hea.replace('.hea', '')\n",
        "        base_file_location_atr = file_location_atr.replace('.atr', '')\n",
        "\n",
        "        st.write(\"### **Preprocessing** \")\n",
        "        record = rdrecord(base_file_location_hea, smooth_frames=True)\n",
        "        signals1 = np.nan_to_num(record.p_signal[:, 0]).tolist()\n",
        "\n",
        "        denoised_signals1 = denoise(signals1)\n",
        "        st.write(\"#### **Phase 1: Data Denoising**\")\n",
        "\n",
        "        def plot_signal(signal, title, filename):\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.title(title)\n",
        "            plt.plot(signal, linewidth=2)\n",
        "            plt.ylabel('Amplitude')\n",
        "            plt.xlabel('Sample number')\n",
        "            plt.savefig(filename)\n",
        "            plt.close()\n",
        "            return filename\n",
        "\n",
        "        noised_signal_plot = plot_signal(signals1[0:256*3], \"Signal before denoising\", \"noised_signal_plot.png\")\n",
        "        denoised_signal_plot = plot_signal(denoised_signals1[0:256*3], \"Signal after denoising\", \"denoised_signal_plot.png\")\n",
        "\n",
        "        denoising_expander1 = st.expander(label='Data denoising')\n",
        "        with denoising_expander1:\n",
        "            st.image(noised_signal_plot, caption='Noised Signal')\n",
        "            st.image(denoised_signal_plot, caption='Denoised Signal')\n",
        "\n",
        "        with st.spinner(\"Executing ...\"):\n",
        "            peaks_signals1, _ = find_peaks(denoised_signals1, distance=150)\n",
        "\n",
        "\n",
        "            for peak in peaks_signals1[1:-1]:\n",
        "                start, end = peak - 128, peak + 128\n",
        "\n",
        "                wave = denoised_signals1[start:end]\n",
        "                ann = rdann(base_file_location_atr, extension='atr', sampfrom=start, sampto=end)\n",
        "                annSymbol = ann.symbol\n",
        "\n",
        "\n",
        "                if not example_beat_printed:\n",
        "                    segmented_heartbeat_plot = plot_signal(wave, \"Segmented heartbeat\", \"segmented_heartbeat.png\")\n",
        "                    example_beat_printed = True\n",
        "\n",
        "                wave = stats.zscore(wave)\n",
        "                if not example_beat_normalized_printed:\n",
        "                    normalized_heartbeat_plot = plot_signal(wave, \"Heartbeat after z-score normalization\", \"normalized_heartbeat.png\")\n",
        "                    example_beat_normalized_printed = True\n",
        "                if len(annSymbol) ==1 and (annSymbol[0] in classes) :\n",
        "                  x_signals1.append(wave)\n",
        "                  # y_signals1.append(arrhythmia_index)\n",
        "\n",
        "            st.write(\"#### **Phase 2: Heartbeat Signal Extraction**\")\n",
        "            leads_expander1 = st.expander(label='Heartbeat Signal Extraction')\n",
        "            with leads_expander1:\n",
        "                st.image(segmented_heartbeat_plot, caption='Segmented heartbeat')\n",
        "\n",
        "            st.write(\"#### **Phase 3: Normalization**\")\n",
        "            normalization_expander = st.expander(label='Normalization')\n",
        "            with normalization_expander:\n",
        "                st.image(normalized_heartbeat_plot, caption='Normalized heartbeat')\n",
        "\n",
        "            st.write(\"#### **Phase 4: Mapping MIT Dataset Classes to AAMI Categories**\")\n",
        "            data1_expander = st.expander(label='Number of samples after the split')\n",
        "            with data1_expander:\n",
        "                st.write(f\"All Data: {len(x_signals1)}\")\n",
        "\n",
        "        x_signals1 = np.array(x_signals1)\n",
        "        st.write(\"### **Classification** \")\n",
        "        # Load the model\n",
        "        selected_model_path = os.path.join(MODEL_DIR, selected_model_file)\n",
        "        model = load_model(selected_model_path)\n",
        "\n",
        "        # Make predictions on the test set\n",
        "        test_x = np.array(x_signals1).reshape(-1, 256, 1)\n",
        "        test_predictions = model.predict(test_x)\n",
        "        test_predictions = np.argmax(test_predictions, axis=1)\n",
        "        predicted_labels = [classes[pred] for pred in test_predictions]\n",
        "\n",
        "        # Get unique classes\n",
        "        unique_classes = list(set(predicted_labels))\n",
        "        classification_expander = st.expander(label='Classification')\n",
        "        with classification_expander:\n",
        "            st.write(\"This ECG contains the following arrhythmias:\")\n",
        "            for label in unique_classes:\n",
        "                st.write(f\"- {class_descriptions[label]} ({label})\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNvQ77vbcUIh",
        "outputId": "67731e37-22d9-4706-ed34-be8e4ba20be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Start Server"
      ],
      "metadata": {
        "id": "rsFUGN5WjPuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from threading import Thread\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Function to run Streamlit\n",
        "def run_streamlit():\n",
        "    os.system('streamlit run /content/app.py --server.port 8501')\n",
        "\n",
        "# Start a thread to run the Streamlit app\n",
        "thread = Thread(target=run_streamlit)\n",
        "thread.start()\n",
        "\n",
        "# Open a tunnel to the Streamlit port 8501\n",
        "public_url = ngrok.connect(addr='8501', proto='http', bind_tls=True)\n",
        "print('Your Streamlit app is live at:', public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSz2FOuDcsR0",
        "outputId": "52adb7b0-1f9a-4d52-8f4e-24901311950c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Streamlit app is live at: NgrokTunnel: \"https://497b-35-185-28-111.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prediction API"
      ],
      "metadata": {
        "id": "KIXnJhl2iUKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colabcode\n",
        "!pip install fastapi"
      ],
      "metadata": {
        "id": "f5Mt7PO8G2BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from colabcode import ColabCode\n",
        "from fastapi import FastAPI"
      ],
      "metadata": {
        "id": "D0qr_6h3ySAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc.run_app(app=app)"
      ],
      "metadata": {
        "id": "BHy_B2CJiRIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "K.set_image_data_format('channels_last')\n",
        "cc = ColabCode(port=12000, code=False)\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from fastapi.responses import FileResponse\n",
        "from scipy import stats\n",
        "from scipy.signal import find_peaks\n",
        "from wfdb import rdrecord\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "import pywt\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "UPLOAD_DIR = \"/content/uploads\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/inception_model.h5\"\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model(\"/content/drive/MyDrive/MLII-model_fold_3.h5\")\n",
        "\n",
        "classes = ['N', 'F', 'Q', 'S', 'V']\n",
        "\n",
        "def denoise(data):\n",
        "    w = pywt.Wavelet('sym4')\n",
        "    maxlev = pywt.dwt_max_level(len(data), w.dec_len)\n",
        "    threshold = 0.04  # Threshold for filtering\n",
        "\n",
        "    coeffs = pywt.wavedec(data, 'sym4', level=maxlev)\n",
        "    for i in range(1, len(coeffs)):\n",
        "        coeffs[i] = pywt.threshold(coeffs[i], threshold * max(coeffs[i]))\n",
        "\n",
        "    datarec = pywt.waverec(coeffs, 'sym4')\n",
        "    return datarec\n",
        "\n",
        "@app.post(\"/uploadfile/\")\n",
        "async def upload_file(file: UploadFile = File(...)):\n",
        "    file_location = os.path.join(UPLOAD_DIR, file.filename)\n",
        "    print(file.filename)\n",
        "    with open(file_location, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "    predictions = await preprocess_and_predict(file_location)\n",
        "    return {\"info\": f\"file '{file.filename}' saved and processed at '{file_location}'\", \"predictions\": predictions}\n",
        "\n",
        "async def preprocess_and_predict(file_path):\n",
        "    try:\n",
        "        num = os.path.splitext(os.path.basename(file_path))[0]\n",
        "        print(num)\n",
        "        record = rdrecord(\"/content/uploads/\"+num, smooth_frames=True, channels=[0, 1])\n",
        "        if record.sig_name[0] != 'MLII':\n",
        "            raise ValueError(\"The primary signal is not 'MLII'.\")\n",
        "\n",
        "        signals0 = np.nan_to_num(record.p_signal[:, 0]).tolist()\n",
        "        signals0 = denoise(signals0)\n",
        "\n",
        "        peaks_signals0, _ = find_peaks(signals0, distance=150)\n",
        "\n",
        "        x_signals0 = []\n",
        "        for peak in peaks_signals0[1:-1]:\n",
        "            start, end = peak - 128, peak + 128\n",
        "            wave = signals0[start:end]\n",
        "\n",
        "            wave = stats.zscore(wave)\n",
        "\n",
        "            x_signals0.append(wave)\n",
        "\n",
        "        # Convert to numpy arrays and reshape for the model\n",
        "        test_x = np.array(x_signals0).reshape(-1, 256, 1)\n",
        "\n",
        "        # Make predictions on the processed signals\n",
        "        test_predictions = model.predict(test_x)\n",
        "        predicted_classes = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "        # Translate predicted classes to corresponding labels\n",
        "        predicted_labels = [classes[pred] for pred in predicted_classes]\n",
        "\n",
        "        # Get unique classes\n",
        "        unique_classes = list(set(predicted_labels))\n",
        "\n",
        "        return unique_classes\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/files/{filename}\")\n",
        "async def get_file(filename: str):\n",
        "    file_location = os.path.join(UPLOAD_DIR, filename)\n",
        "    if os.path.exists(file_location):\n",
        "        return FileResponse(path=file_location, filename=filename)\n",
        "    return {\"error\": \"File not found\"}\n",
        "\n",
        "# Since Colab already has an event loop, we use this method to run the app\n",
        "import threading\n",
        "\n",
        "def run_app():\n",
        "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n",
        "    server = uvicorn.Server(config)\n",
        "    server.run()\n",
        "\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()\n"
      ],
      "metadata": {
        "id": "yzQxSq-J6G65"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_OLr1TtDhZf_",
        "KZivx1EYhzU5",
        "C6-R1on-jA-p",
        "y8hr_ZjgipEl",
        "rsFUGN5WjPuo",
        "KIXnJhl2iUKg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}